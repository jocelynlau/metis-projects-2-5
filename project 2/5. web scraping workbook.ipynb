{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. web scraping workbook\n",
    "Purpose: To demonstrate web scraping abilities since no web scraping was done for Project 2. Code can be used to import tables from linked pages that share a common source page. Example is to pull tables of the list of hospitals in each state (found on separate pages) from wikipedia's source page for list of all hospitals by state. Code assumes the table of interest is the first table. Code saves tables as pickles.\n",
    "\n",
    "Option 1: pull tables using HTML reference<br>\n",
    "Option 2: pull tables using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# if needed: pip install requests or conda install requests\n",
    "import requests\n",
    "\n",
    "requests.__path__\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import chromedriver_binary\n",
    "\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and pull data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://en.wikipedia.org/wiki/Lists_of_hospitals_in_the_United_States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of states of interest\n",
    "state_list = ['Alabama','Connecticut','Idaho']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Pull tables using HTML reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in state_list:\n",
    "    #click on list\n",
    "    driver.find_element_by_link_text(state).click()\n",
    "    #pull table info into list using beautiful soup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    #get table items\n",
    "    tables = soup.find_all('table')\n",
    "    rows = [row for row in tables[0].find_all('tr')]\n",
    "    headers = [x.getText().strip() for x in tables[0].find_all('th')]\n",
    "    \n",
    "    #pull table data into dataframe\n",
    "    for i in range(len(headers)): \n",
    "    #first column\n",
    "        if i == 0:\n",
    "            df = []\n",
    "            for row in range(1,len(rows)):\n",
    "                df.append(rows[row].find_all('td')[i].text.strip())\n",
    "            df = pd.DataFrame(df)\n",
    "        else:\n",
    "            new_col = []\n",
    "            for row in range(1,len(rows)):\n",
    "                new_col.append(rows[row].find_all('td')[i].text.strip())\n",
    "            df[i] = new_col\n",
    "    \n",
    "    #save out dataframe\n",
    "    headers_dict = dict(list(zip(range(len(headers)),headers)))\n",
    "    df.rename(columns=headers_dict,inplace=True)\n",
    "    df['State'] = state \n",
    "    \n",
    "    with open('web scraping/'+state+'.pickle', 'wb') as to_write:\n",
    "        pickle.dump(df, to_write)\n",
    "    \n",
    "    #return to main page\n",
    "    driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Pull tables using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in state_list:\n",
    "    #click on list\n",
    "    driver.find_element_by_link_text(state).click()\n",
    "\n",
    "    tables = pd.read_html(driver.current_url)\n",
    "    df = tables[0]\n",
    "    df['State'] = state \n",
    "    \n",
    "    with open('web scraping/'+state+'.pickle', 'wb') as to_write:\n",
    "        pickle.dump(df, to_write)\n",
    "    \n",
    "    #return to main page\n",
    "    driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
